{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import libs.text_preprocess as tp\n",
    "import libs.genetic_algorithm as ga\n",
    "from libs.TSNMF_Class import TSNMF\n",
    "\n",
    "# https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_prepare.py\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "\n",
    "import os\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Run one of the cells below\n",
    "\n",
    "* index\n",
    "* theme\n",
    "* text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data['theme'] = data['theme'].apply(lambda x: [x])\n",
    "    data = data.sort_values('theme')\n",
    "    data = data[['title', 'theme', 'text']]\n",
    "    data = data.rename({'title': 'id'}, axis=1)\n",
    "    \n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(' '.join(reuters.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = brown.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(brown.categories(file))\n",
    "    text.append(' '.join(brown.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Texts\n",
    "1. Fix bad wording: isn't -> is not\n",
    "2. Clean and Tokenize -> min word len = 3, tokenize\n",
    "3. Stopword Removal -> nltk.stopwords\n",
    "4. Lemmatization -> WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(tp.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>theme</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cn05</td>\n",
       "      <td>[adventure]</td>\n",
       "      <td>carry quirt start raise let fall dangle wrist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cn17</td>\n",
       "      <td>[adventure]</td>\n",
       "      <td>guide divan turn face sit quietly star wide ey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cn16</td>\n",
       "      <td>[adventure]</td>\n",
       "      <td>rattle fender hum tire chatter gear charm melo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cn14</td>\n",
       "      <td>[adventure]</td>\n",
       "      <td>large dutch spring mine supply town appearance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cn13</td>\n",
       "      <td>[adventure]</td>\n",
       "      <td>shoulder could see max loose grin burnside glo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id        theme                                               text\n",
       "0  cn05  [adventure]  carry quirt start raise let fall dangle wrist ...\n",
       "1  cn17  [adventure]  guide divan turn face sit quietly star wide ey...\n",
       "2  cn16  [adventure]  rattle fender hum tire chatter gear charm melo...\n",
       "3  cn14  [adventure]  large dutch spring mine supply town appearance...\n",
       "4  cn13  [adventure]  shoulder could see max loose grin burnside glo..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = data[data['text'].apply(lambda x: x.count(' ') > 25)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(dataX), len(sorted(list(set(dataX['theme'].sum()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i, row in dataX.iterrows():\n",
    "    for t in row['theme']:\n",
    "        if t not in labels:\n",
    "            labels[t] = 1\n",
    "        else:\n",
    "            labels[t] += 1\n",
    "            \n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: x.count(' ') > 25)].reset_index(drop=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF-LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.genetic_algorithm as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_keys = ['type', 'dataset','batch_size','theme_count','train_set_size','test_set_size','train_split', 'term_vectorizer','max_score_one_theme',\n",
    "                    'max_score','prediction_score','prediction_score_perc','prediction_score_mean', 'prediction_score_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reuters_lda_tf_10.pickle\n",
      "1 reuters_lda_tf_20.pickle\n",
      "2 reuters_lda_tf_30.pickle\n",
      "3 reuters_lda_tf_40.pickle\n",
      "4 reuters_lda_tf_50.pickle\n",
      "5 reuters_lda_tf_60.pickle\n",
      "6 reuters_lda_tf_70.pickle\n",
      "7 reuters_lda_tf_80.pickle\n",
      "8 reuters_lda_tf_90.pickle\n",
      "9 reuters_nmf_kl_tf_10.pickle\n",
      "10 reuters_nmf_kl_tf_20.pickle\n",
      "11 reuters_nmf_kl_tf_30.pickle\n",
      "12 reuters_nmf_kl_tf_40.pickle\n",
      "13 reuters_nmf_kl_tf_50.pickle\n",
      "14 reuters_nmf_kl_tf_60.pickle\n",
      "15 reuters_nmf_kl_tf_70.pickle\n",
      "16 reuters_nmf_kl_tf_80.pickle\n",
      "17 reuters_nmf_kl_tf_90.pickle\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=result_dict_keys)\n",
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_reuters_nmflda\")):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_reuters_nmflda/\"+pickle_name,\"rb\")\n",
    "    nmflda_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    themes = sorted(list(set(nmflda_context_list[0]['data']['theme'].sum())))\n",
    "    n_themes = len(themes)\n",
    "    result_dict = {\n",
    "        'type': nmflda_context_list[0]['type'],\n",
    "        'dataset': pickle_name[:pickle_name.find('_')],\n",
    "        'batch_size': len(nmflda_context_list),\n",
    "        'theme_count': n_themes,\n",
    "        'train_set_size': len(nmflda_context_list[0]['data'][nmflda_context_list[0]['data'].labeled == 1]),\n",
    "        'test_set_size': len(nmflda_context_list[0]['data'][nmflda_context_list[0]['data'].labeled == 0]),\n",
    "        'train_split': nmflda_context_list[0]['train_perc'],\n",
    "        'term_vectorizer': nmflda_context_list[0]['term_vectorizer'],\n",
    "        'max_score_one_theme': np.log(n_themes),\n",
    "        'max_score': 0,\n",
    "        'prediction_score': 0,\n",
    "        'prediction_score_perc': 0,\n",
    "        'prediction_score_mean': 0,\n",
    "        'prediction_score_std': 0\n",
    "    }\n",
    "\n",
    "    prediction_scores = []\n",
    "    for nmflda_context in nmflda_context_list:\n",
    "        temp_prediction_scores, _ = ga.calculateTestScore(nmflda_context['solution'], nmflda_context['data'][nmflda_context['data'].labeled == 0], nmflda_context['W'], themes)\n",
    "        prediction_scores.append(temp_prediction_scores)\n",
    "    prediction_scores = np.array(prediction_scores)\n",
    "\n",
    "    for i in range(len(prediction_scores)):\n",
    "        max_score = sum([sum(np.log(n_themes)-np.log(range(1, len(aps)+1))) for aps in nmflda_context_list[i]['data'][nmflda_context_list[i]['data'].labeled == 0]['theme']])\n",
    "        result_dict['max_score'] += max_score/len(nmflda_context_list)\n",
    "        result_dict['prediction_score'] += prediction_scores[i].sum()/len(nmflda_context_list)\n",
    "        result_dict['prediction_score_perc'] += 100*prediction_scores[i].sum()/max_score/len(nmflda_context_list)\n",
    "        result_dict['prediction_score_mean'] += prediction_scores[i].mean()/len(nmflda_context_list)\n",
    "        result_dict['prediction_score_std'] += prediction_scores[i].std()/len(nmflda_context_list)\n",
    "\n",
    "    result_df = result_df.append(result_dict, ignore_index=True)\n",
    "result_df.to_excel(pickle_name[:pickle_name.find('_')]+'_nmflda_scoring.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNMF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_score(tsnmf_context_list):\n",
    "    themes = tsnmf_context_list[0]['tsnmf_model'].themes\n",
    "    \n",
    "    prediction_scores = []\n",
    "    prediction_scores_log = []\n",
    "    prediction_scores_rev_log = []\n",
    "    for tsnmf_context in tsnmf_context_list:\n",
    "        temp_pred_scores = []\n",
    "        temp_pred_scores_log = []\n",
    "        temp_pred_scores_rev_log = []\n",
    "        for ind, doc_wth in enumerate(tsnmf_context['W_test_high']):\n",
    "            temp_pred_score = []\n",
    "            temp_pred_score_log = []\n",
    "            temp_pred_score_rev_log = []\n",
    "            for theme in tsnmf_context['tsnmf_model'].test_data.iloc[ind]['theme']:\n",
    "                theme_id = themes.index(theme)\n",
    "                temp_pred_score.append(np.argwhere(doc_wth.argsort()==theme_id)[0][0] + 1)\n",
    "                temp_pred_score_log.append(np.log(np.argwhere(doc_wth.argsort()==theme_id)[0][0] + 1))\n",
    "                temp_pred_score_rev_log.append(np.log(len(themes))-np.log(np.argwhere(doc_wth.argsort()[::-1]==theme_id)[0][0] + 1))\n",
    "            temp_pred_scores.append(temp_pred_score)\n",
    "            temp_pred_scores_log.append(temp_pred_score_log)\n",
    "            temp_pred_scores_rev_log.append(temp_pred_score_rev_log)\n",
    "\n",
    "        prediction_scores.append(np.array([sum(tps) for tps in temp_pred_scores]))\n",
    "        prediction_scores_log.append(np.array([sum(tps) for tps in temp_pred_scores_log]))\n",
    "        prediction_scores_rev_log.append(np.array([sum(tps) for tps in temp_pred_scores_rev_log]))\n",
    "\n",
    "    return np.array(prediction_scores), np.array(prediction_scores_log), np.array(prediction_scores_rev_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_keys = ['dataset','batch_size','theme_count','train_set_size','test_set_size','train_split','supervision','separate_models','bCool_init',\n",
    "                    'beta_loss','term_vectorizer','max_score_one_theme','max_score','prediction_score','prediction_score_perc','prediction_score_mean',\n",
    "                    'prediction_score_std','max_log_score_one_theme','max_log_score','prediction_log_score','prediction_log_score_perc',\n",
    "                    'prediction_log_score_mean','prediction_log_score_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_10.pickle\n",
      "1 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_20.pickle\n",
      "2 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_30.pickle\n",
      "3 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_40.pickle\n",
      "4 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_50.pickle\n",
      "5 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_60.pickle\n",
      "6 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_70.pickle\n",
      "7 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_80.pickle\n",
      "8 schwartz_topic1_semi_supervised_combined_bCool_kullback-leibler_tf_90.pickle\n",
      "9 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_10.pickle\n",
      "10 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_20.pickle\n",
      "11 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_30.pickle\n",
      "12 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_40.pickle\n",
      "13 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_50.pickle\n",
      "14 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_60.pickle\n",
      "15 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_70.pickle\n",
      "16 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_80.pickle\n",
      "17 schwartz_topic1_semi_supervised_combined_random_kullback-leibler_tf_90.pickle\n",
      "18 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_10.pickle\n",
      "19 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_20.pickle\n",
      "20 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_30.pickle\n",
      "21 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_40.pickle\n",
      "22 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_50.pickle\n",
      "23 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_60.pickle\n",
      "24 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_70.pickle\n",
      "25 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_80.pickle\n",
      "26 schwartz_topic1_semi_supervised_separate_bCool_kullback-leibler_tf_90.pickle\n",
      "27 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_10.pickle\n",
      "28 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_20.pickle\n",
      "29 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_30.pickle\n",
      "30 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_40.pickle\n",
      "31 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_50.pickle\n",
      "32 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_60.pickle\n",
      "33 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_70.pickle\n",
      "34 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_80.pickle\n",
      "35 schwartz_topic1_semi_supervised_separate_random_kullback-leibler_tf_90.pickle\n",
      "36 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_10.pickle\n",
      "37 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_20.pickle\n",
      "38 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_30.pickle\n",
      "39 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_40.pickle\n",
      "40 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_50.pickle\n",
      "41 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_60.pickle\n",
      "42 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_70.pickle\n",
      "43 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_80.pickle\n",
      "44 schwartz_topic1_supervised_combined_bCool_kullback-leibler_tf_90.pickle\n",
      "45 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_10.pickle\n",
      "46 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_20.pickle\n",
      "47 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_30.pickle\n",
      "48 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_40.pickle\n",
      "49 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_50.pickle\n",
      "50 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_60.pickle\n",
      "51 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_70.pickle\n",
      "52 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_80.pickle\n",
      "53 schwartz_topic1_supervised_combined_random_kullback-leibler_tf_90.pickle\n",
      "54 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_10.pickle\n",
      "55 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_20.pickle\n",
      "56 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_30.pickle\n",
      "57 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_40.pickle\n",
      "58 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_50.pickle\n",
      "59 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_60.pickle\n",
      "60 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_70.pickle\n",
      "61 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_80.pickle\n",
      "62 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_90.pickle\n",
      "63 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_10.pickle\n",
      "64 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_20.pickle\n",
      "65 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_30.pickle\n",
      "66 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_40.pickle\n",
      "67 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_50.pickle\n",
      "68 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_60.pickle\n",
      "69 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_70.pickle\n",
      "70 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_80.pickle\n",
      "71 schwartz_topic1_supervised_separate_random_kullback-leibler_tf_90.pickle\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=result_dict_keys)\n",
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_schwartz_topic1\")):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_schwartz_topic1/\"+pickle_name,\"rb\")\n",
    "    tsnmf_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    prediction_scores, prediction_scores_log, prediction_scores_rev_log = calculate_test_score(tsnmf_context_list)\n",
    "    \n",
    "    n_themes = len(tsnmf_context_list[0]['tsnmf_model'].themes)\n",
    "\n",
    "    result_dict = {\n",
    "        'dataset': pickle_name[:pickle_name.find('_')],\n",
    "        'batch_size': len(tsnmf_context_list),\n",
    "        'theme_count': n_themes,\n",
    "        'train_set_size': len(tsnmf_context_list[0]['tsnmf_model'].train_data),\n",
    "        'test_set_size': len(tsnmf_context_list[0]['tsnmf_model'].test_data),\n",
    "        'train_split': tsnmf_context_list[0]['tsnmf_model'].train_test_split[0],\n",
    "        'supervision': tsnmf_context_list[0]['tsnmf_model'].supervision,\n",
    "        'separate_models': 'separate' if tsnmf_context_list[0]['tsnmf_model'].separate_models else 'combined',\n",
    "        'bCool_init': 'bCool' if tsnmf_context_list[0]['tsnmf_model'].bCool_init else 'random',\n",
    "        'beta_loss': tsnmf_context_list[0]['tsnmf_model'].beta_loss,\n",
    "        'term_vectorizer': tsnmf_context_list[0]['tsnmf_model'].term_vectorizer,\n",
    "        'max_score_one_theme': n_themes,\n",
    "        'max_score': 0,\n",
    "        'prediction_score': 0,\n",
    "        'prediction_score_perc': 0,\n",
    "        'prediction_score_mean': 0,\n",
    "        'prediction_score_std': 0,\n",
    "        'max_log_score_one_theme': np.log(n_themes),\n",
    "        'max_log_score': 0,\n",
    "        'prediction_log_score': 0,\n",
    "        'prediction_log_score_perc': 0,\n",
    "        'prediction_log_score_mean': 0,\n",
    "        'prediction_log_score_std': 0,\n",
    "        'max_rev_log_score': 0,\n",
    "        'prediction_rev_log_score': 0,\n",
    "        'prediction_rev_log_score_perc': 0,\n",
    "        'prediction_rev_log_score_mean': 0,\n",
    "        'prediction_rev_log_score_std': 0\n",
    "    }\n",
    "\n",
    "    for i in range(len(prediction_scores)):\n",
    "        max_score = sum([sum(range(n_themes-len(aps)+1, n_themes+1)) for aps in tsnmf_context_list[i]['tsnmf_model'].test_data['theme']])\n",
    "        result_dict['max_score'] += max_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_score'] += prediction_scores[i].sum()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_score_perc'] += 100*prediction_scores[i].sum()/max_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_score_mean'] += prediction_scores[i].mean()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_score_std'] += prediction_scores[i].std()/len(tsnmf_context_list)\n",
    "\n",
    "        max_log_score = sum([sum(np.log(range(n_themes-len(aps)+1, n_themes+1))) for aps in tsnmf_context_list[i]['tsnmf_model'].test_data['theme']])\n",
    "        result_dict['max_log_score'] += max_log_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_log_score'] += prediction_scores_log[i].sum()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_log_score_perc'] += 100*prediction_scores_log[i].sum()/max_log_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_log_score_mean'] += prediction_scores_log[i].mean()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_log_score_std'] += prediction_scores_log[i].std()/len(tsnmf_context_list)\n",
    "        \n",
    "        max_rev_log_score = sum([sum(np.log(n_themes)-np.log(range(1, len(aps)+1))) for aps in tsnmf_context_list[i]['tsnmf_model'].test_data['theme']])\n",
    "        result_dict['max_rev_log_score'] += max_rev_log_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_rev_log_score'] += prediction_scores_rev_log[i].sum()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_rev_log_score_perc'] += 100*prediction_scores_rev_log[i].sum()/max_rev_log_score/len(tsnmf_context_list)\n",
    "        result_dict['prediction_rev_log_score_mean'] += prediction_scores_rev_log[i].mean()/len(tsnmf_context_list)\n",
    "        result_dict['prediction_rev_log_score_std'] += prediction_scores_rev_log[i].std()/len(tsnmf_context_list)\n",
    "        \n",
    "    result_df = result_df.append(result_dict, ignore_index=True)\n",
    "result_df.to_excel(pickle_name[:pickle_name.find('_')]+'_scoring.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datathon]",
   "language": "python",
   "name": "conda-env-datathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
