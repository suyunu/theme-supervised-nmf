{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import libs.text_preprocess as tp\n",
    "import libs.genetic_algorithm as ga\n",
    "from libs.TSNMF_Class import TSNMF\n",
    "\n",
    "import pickle\n",
    "\n",
    "# https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_prepare.py\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Run one of the cells below\n",
    "\n",
    "* index\n",
    "* theme\n",
    "* text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data['theme'] = data['theme'].apply(lambda x: [x])\n",
    "    data = data.sort_values('theme')\n",
    "    data = data[['title', 'theme', 'text']]\n",
    "    data = data.rename({'title': 'id'}, axis=1)\n",
    "    \n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(' '.join(reuters.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = brown.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(brown.categories(file))\n",
    "    text.append(' '.join(brown.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Texts\n",
    "1. Fix bad wording: isn't -> is not\n",
    "2. Clean and Tokenize -> min word len = 3, tokenize\n",
    "3. Stopword Removal -> nltk.stopwords\n",
    "4. Lemmatization -> WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(tp.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>theme</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ascribed status</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>ascribe status part series political legal ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relational capital</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>relational capital redirect relational capital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human resources</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>human resource us see human resource disambigu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intellectual capital</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>intellectual capital intellectual capital inta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expenses versus Capital Expenditures</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>expense versus capital expenditure redirect ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          theme  \\\n",
       "0                       Ascribed status  [achievement]   \n",
       "1                    Relational capital  [achievement]   \n",
       "2                       Human resources  [achievement]   \n",
       "3                  Intellectual capital  [achievement]   \n",
       "4  Expenses versus Capital Expenditures  [achievement]   \n",
       "\n",
       "                                                text  \n",
       "0  ascribe status part series political legal ant...  \n",
       "1  relational capital redirect relational capital...  \n",
       "2  human resource us see human resource disambigu...  \n",
       "3  intellectual capital intellectual capital inta...  \n",
       "4  expense versus capital expenditure redirect ca...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: x.count(' ') > 25)].reset_index(drop=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pickle_name(tsnmf_model, dataset_name):\n",
    "    pickle_name = dataset_name\n",
    "    pickle_name += '_'\n",
    "    pickle_name += tsnmf_model.supervision\n",
    "    pickle_name += '_'\n",
    "    pickle_name += 'separate' if tsnmf_model.separate_models else 'combined'\n",
    "    pickle_name += '_'\n",
    "    pickle_name += 'bCool' if tsnmf_model.bCool_init else 'random'\n",
    "    pickle_name += '_'\n",
    "    pickle_name += tsnmf_model.beta_loss\n",
    "    pickle_name += '_'\n",
    "    pickle_name += tsnmf_model.term_vectorizer\n",
    "    pickle_name += '_'\n",
    "    pickle_name += str(int(tsnmf_model.train_test_split[0]*100))\n",
    "    \n",
    "    return pickle_name\n",
    "\n",
    "def save_to_pickle(tsnmf_context_list, pickle_name):\n",
    "    pickle_out = open(\"all_pickles/pickles_schwartz_terms/\"+pickle_name+\".pickle\",\"wb\")\n",
    "    pickle.dump(tsnmf_context_list, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervision_list = ['semi_supervised', 'supervised']\n",
    "separate_models_list = [False, True]\n",
    "bCool_init_list = [False, True]\n",
    "beta_loss_list = ['kullback-leibler']\n",
    "term_vectorizer_list = ['tf']\n",
    "train_test_split_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "random_state_list = [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervision_list = ['supervised']\n",
    "separate_models_list = [True]\n",
    "bCool_init_list = [True]\n",
    "beta_loss_list = ['kullback-leibler']\n",
    "term_vectorizer_list = ['tf']\n",
    "train_test_split_list = [1]\n",
    "random_state_list = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schwartz_topic_supervised_separate_bCool_kullback-leibler_tf_100 1\n",
      "Training.. 51.84s - Testing.. 0.00s - Done:  51.86s\n"
     ]
    }
   ],
   "source": [
    "for supervision in supervision_list:\n",
    "    for separate_models in separate_models_list:\n",
    "        for bCool_init in bCool_init_list:\n",
    "            for beta_loss in beta_loss_list:\n",
    "                for term_vectorizer in term_vectorizer_list:\n",
    "                    for train_perc in train_test_split_list:\n",
    "                        tsnmf_context_list = []\n",
    "                        for random_state in random_state_list:\n",
    "                            tsnmf_model = TSNMF(data = data, supervision = supervision, separate_models = separate_models, bCool_init = bCool_init,\n",
    "                                                train_test_split = [train_perc, 1-train_perc], n_topics = 3, n_terms = 10000,background_for_theme = True,\n",
    "                                                background_scoring = True, beta_loss = beta_loss, term_vectorizer = term_vectorizer, random_state = random_state)\n",
    "                            \n",
    "                            pickle_name = get_pickle_name(tsnmf_model, 'schwartz_topic')\n",
    "                            t0 = time()\n",
    "                            print(pickle_name, random_state)\n",
    "                            \n",
    "                            if term_vectorizer == 'tfidf' and beta_loss == 'frobenius':\n",
    "                                print(\"continue\")\n",
    "                                continue\n",
    "                                \n",
    "                            tsnmf_model = tsnmf_model.split_train_test()\n",
    "                            \n",
    "#                             # For Reuters\n",
    "#                             if train_perc > 0.25 and train_perc < 0.75:\n",
    "#                                 tsnmf_model = tsnmf_model.split_train_test()\n",
    "#                             else:\n",
    "#                                 tsnmf_model = tsnmf_model.split_train_test_forced()\n",
    "                            \n",
    "                            print(\"Training.. \", end='')\n",
    "                            t1 = time()\n",
    "                            tsnmf_context = tsnmf_model.fit()\n",
    "                            print(\"%0.2fs - \" % (time() - t1), end='')\n",
    "                            \n",
    "                            print(\"Testing.. \", end='')\n",
    "                            t1 = time()\n",
    "#                             tsnmf_context = tsnmf_model.evaluate_test_corpus(tsnmf_context)\n",
    "                            print(\"%0.2fs - \" % (time() - t1), end='')\n",
    "                            \n",
    "                            tsnmf_context['tsnmf_model'] = tsnmf_model\n",
    "                            tsnmf_context_list.append(tsnmf_context)\n",
    "                            print(\"Done:  %0.2fs\" % (time() - t0))\n",
    "                            \n",
    "                        if not (term_vectorizer == 'tfidf' and beta_loss == 'frobenius'):\n",
    "                            save_to_pickle(tsnmf_context_list, pickle_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic NMF-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train NMF and LDA using all the data  points then test the scores of the test data\n",
    "* Other version could be using only train data to train the model and using transfor to get W matrix for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.genetic_algorithm as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reuters_lda_tf_10 5 - 90\n",
      "2022.2671098588698 274.8381748199463\n",
      "reuters_lda_tf_10 6 - 90\n",
      "1611.6321233406484 282.2373912334442\n",
      "reuters_lda_tf_20 5 - 90\n",
      "3816.056392922948 363.3355665206909\n",
      "reuters_lda_tf_20 6 - 90\n",
      "2868.3257246428034 373.8933324813843\n",
      "reuters_lda_tf_30 5 - 4547.532682205725 452.85126280784607\n",
      "reuters_lda_tf_30 6 - 4989.437844178909 459.78307580947876\n",
      "reuters_lda_tf_40 5 - 7462.52380374062 564.7877328395844\n",
      "reuters_lda_tf_40 6 - 5339.851872636042 551.7591743469238\n",
      "reuters_lda_tf_50 5 - 7498.295282242676 632.3538200855255\n",
      "reuters_lda_tf_50 6 - 7528.2774544532185 630.1147961616516\n",
      "reuters_lda_tf_60 5 - 8959.569442807733 717.9123907089233\n",
      "reuters_lda_tf_60 6 - 9130.884343570327 745.151282787323\n",
      "reuters_lda_tf_70 5 - 12171.159545015074 832.2895488739014\n",
      "reuters_lda_tf_70 6 - 10777.684544381093 846.8187634944916\n",
      "reuters_lda_tf_80 5 - 90\n",
      "14706.876212814364 888.6425547599792\n",
      "reuters_lda_tf_80 6 - 90\n",
      "12900.537466963138 876.054221868515\n",
      "reuters_lda_tf_90 5 - 90\n",
      "16357.570602643982 944.3966774940491\n",
      "reuters_lda_tf_90 6 - 90\n",
      "13123.307891979044 969.4132835865021\n"
     ]
    }
   ],
   "source": [
    "themes = sorted(list(set(data['theme'].sum())))\n",
    "train_test_split_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#random_state_list = [1,2,3,4,5]\n",
    "random_state_list = [5,6]\n",
    "\n",
    "for train_perc in train_test_split_list:\n",
    "    pickle_context_list = []\n",
    "    #pickle_name = 'reuters_nmf_kl_tf_'+str(int(train_perc*100))\n",
    "    pickle_name = 'reuters_lda_tf_'+str(int(train_perc*100))\n",
    "    for random_state in random_state_list:\n",
    "        print(pickle_name, random_state,  end=' - ')\n",
    "        t0 = time()\n",
    "\n",
    "        tsnmf_model = TSNMF(data = data, supervision = 'semi_supervised', train_test_split = [train_perc, 1-train_perc], random_state = random_state)\n",
    "        \n",
    "        if train_perc > 0.25 and train_perc < 0.75:\n",
    "            tsnmf_model = tsnmf_model.split_train_test()\n",
    "        else:\n",
    "            tsnmf_model = tsnmf_model.split_train_test_forced()\n",
    "\n",
    "        train_data = tsnmf_model.train_data.copy(deep=True)\n",
    "        test_data = tsnmf_model.test_data.copy(deep=True)\n",
    "\n",
    "        corpus = list(train_data.text)\n",
    "        term_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "        #term_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "        tf = term_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        #model = NMF(n_components = len(themes), solver='mu', beta_loss='kullback-leibler', alpha=.1, l1_ratio=.5)\n",
    "        #W = model.fit_transform(X=tf)\n",
    "\n",
    "        model = LDA(n_components=len(themes))\n",
    "        W = model.fit_transform(tf)\n",
    "\n",
    "        population, solution, solution_ind, solution_obj = ga.run_ga(train_data[train_data.labeled == 1], W, themes, stopGeneration=100)\n",
    "        print(solution_obj, time()-t0)\n",
    "\n",
    "        pickle_context = {\n",
    "            'type': 'LDA',\n",
    "            'model': model,\n",
    "            'W': W,\n",
    "            'solution': solution,\n",
    "            'solution_obj': solution_obj,\n",
    "            'data': train_data,\n",
    "            'beta_loss': 'none',\n",
    "            'term_vectorizer': 'tf',\n",
    "            'train_perc': train_perc,\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        pickle_context_list.append(pickle_context)\n",
    "        \n",
    "    pickle_out = open(\"all_pickles/pickles_reuters_nmflda2/\"+pickle_name+\".pickle\",\"wb\")\n",
    "    pickle.dump(pickle_context_list, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schwartz_nmf_kl_tf_100 1 - 529.226235753119 43.60194253921509\n"
     ]
    }
   ],
   "source": [
    "themes = sorted(list(set(data['theme'].sum())))\n",
    "train_test_split_list = [1]\n",
    "#random_state_list = [1,2,3,4,5]\n",
    "random_state_list = [1]\n",
    "\n",
    "for train_perc in train_test_split_list:\n",
    "    pickle_context_list = []\n",
    "    pickle_name = 'schwartz_nmf_kl_tf_'+str(int(train_perc*100))\n",
    "#     pickle_name = 'schwartz_lda_tf_'+str(int(train_perc*100))\n",
    "    for random_state in random_state_list:\n",
    "        print(pickle_name, random_state,  end=' - ')\n",
    "        t0 = time()\n",
    "\n",
    "        tsnmf_model = TSNMF(data = data, supervision = 'semi_supervised', train_test_split = [train_perc, 1-train_perc], random_state = random_state)\n",
    "        \n",
    "        if train_perc > 0.25 and train_perc < 0.75:\n",
    "            tsnmf_model = tsnmf_model.split_train_test()\n",
    "        else:\n",
    "            tsnmf_model = tsnmf_model.split_train_test_forced()\n",
    "\n",
    "        train_data = tsnmf_model.train_data.copy(deep=True)\n",
    "        test_data = tsnmf_model.test_data.copy(deep=True)\n",
    "\n",
    "        corpus = list(train_data.text)\n",
    "        term_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "        #term_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "        tf = term_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        model = NMF(n_components = len(themes), solver='mu', beta_loss='kullback-leibler', alpha=.1, l1_ratio=.5)\n",
    "        W = model.fit_transform(X=tf)\n",
    "\n",
    "#         model = LDA(n_components=len(themes))\n",
    "#         W = model.fit_transform(tf)\n",
    "\n",
    "        population, solution, solution_ind, solution_obj = ga.run_ga(train_data[train_data.labeled == 1], W, themes, stopGeneration=100)\n",
    "        print(solution_obj, time()-t0)\n",
    "\n",
    "        pickle_context = {\n",
    "            'type': 'NMF',\n",
    "            'model': model,\n",
    "            'W': W,\n",
    "            'solution': solution,\n",
    "            'solution_obj': solution_obj,\n",
    "            'data': train_data,\n",
    "            'beta_loss': 'none',\n",
    "            'term_vectorizer': 'tf',\n",
    "            'train_perc': train_perc,\n",
    "            'random_state': random_state\n",
    "        }\n",
    "        pickle_context_list.append(pickle_context)\n",
    "        \n",
    "    pickle_out = open(\"all_pickles/pickles_schwartz_terms/\"+pickle_name+\".pickle\",\"wb\")\n",
    "    pickle.dump(pickle_context_list, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datathon]",
   "language": "python",
   "name": "conda-env-datathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
