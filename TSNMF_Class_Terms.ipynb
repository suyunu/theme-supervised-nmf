{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import libs.text_preprocess as tp\n",
    "import libs.genetic_algorithm as ga\n",
    "from libs.TSNMF_Class import TSNMF\n",
    "\n",
    "# https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_prepare.py\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "\n",
    "import os\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Run one of the cells below\n",
    "\n",
    "* index\n",
    "* theme\n",
    "* text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data['theme'] = data['theme'].apply(lambda x: [x])\n",
    "    data = data.sort_values('theme')\n",
    "    data = data[['title', 'theme', 'text']]\n",
    "    data = data.rename({'title': 'id'}, axis=1)\n",
    "    \n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(' '.join(reuters.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = brown.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(brown.categories(file))\n",
    "    text.append(' '.join(brown.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Texts\n",
    "1. Fix bad wording: isn't -> is not\n",
    "2. Clean and Tokenize -> min word len = 3, tokenize\n",
    "3. Stopword Removal -> nltk.stopwords\n",
    "4. Lemmatization -> WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(tp.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>theme</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ascribed status</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>ascribe status part series political legal ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Relational capital</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>relational capital redirect relational capital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human resources</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>human resource us see human resource disambigu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intellectual capital</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>intellectual capital intellectual capital inta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expenses versus Capital Expenditures</td>\n",
       "      <td>[achievement]</td>\n",
       "      <td>expense versus capital expenditure redirect ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id          theme  \\\n",
       "0                       Ascribed status  [achievement]   \n",
       "1                    Relational capital  [achievement]   \n",
       "2                       Human resources  [achievement]   \n",
       "3                  Intellectual capital  [achievement]   \n",
       "4  Expenses versus Capital Expenditures  [achievement]   \n",
       "\n",
       "                                                text  \n",
       "0  ascribe status part series political legal ant...  \n",
       "1  relational capital redirect relational capital...  \n",
       "2  human resource us see human resource disambigu...  \n",
       "3  intellectual capital intellectual capital inta...  \n",
       "4  expense versus capital expenditure redirect ca...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = data[data['text'].apply(lambda x: x.count(' ') > 25)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 433, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), len(dataX), len(sorted(list(set(dataX['theme'].sum()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for i, row in dataX.iterrows():\n",
    "    for t in row['theme']:\n",
    "        if t not in labels:\n",
    "            labels[t] = 1\n",
    "        else:\n",
    "            labels[t] += 1\n",
    "            \n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: x.count(' ') > 25)].reset_index(drop=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF-LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 reuters_lda_tf_10.pickle\n",
      "1 reuters_lda_tf_20.pickle\n",
      "2 reuters_lda_tf_30.pickle\n",
      "3 reuters_lda_tf_40.pickle\n",
      "4 reuters_lda_tf_50.pickle\n",
      "5 reuters_lda_tf_60.pickle\n",
      "6 reuters_lda_tf_70.pickle\n",
      "7 reuters_lda_tf_80.pickle\n",
      "8 reuters_lda_tf_90.pickle\n",
      "9 reuters_nmf_kl_tf_10.pickle\n",
      "10 reuters_nmf_kl_tf_20.pickle\n",
      "11 reuters_nmf_kl_tf_30.pickle\n",
      "12 reuters_nmf_kl_tf_40.pickle\n",
      "13 reuters_nmf_kl_tf_50.pickle\n",
      "14 reuters_nmf_kl_tf_60.pickle\n",
      "15 reuters_nmf_kl_tf_70.pickle\n",
      "16 reuters_nmf_kl_tf_80.pickle\n",
      "17 reuters_nmf_kl_tf_90.pickle\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame(columns=result_dict_keys)\n",
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_schwartz_nmflda\")):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_schwartz_nmflda/\"+pickle_name,\"rb\")\n",
    "    nmflda_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    themes = sorted(list(set(nmflda_context_list[0]['data']['theme'].sum())))\n",
    "    n_themes = len(themes)\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNMF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 schwartz_semi_supervised_combined_bCool_kullback-leibler_tf_10.pickle\n"
     ]
    }
   ],
   "source": [
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_schwartz\")):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_schwartz/\"+pickle_name,\"rb\")\n",
    "    tsnmf_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'schwartz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_score(tsnmf_context_list):\n",
    "    themes = tsnmf_context_list[0]['tsnmf_model'].themes\n",
    "    \n",
    "    prediction_scores = []\n",
    "    for tsnmf_context in tsnmf_context_list:\n",
    "        temp_pred_scores = []\n",
    "        for ind, doc_wth in enumerate(tsnmf_context['W_test_high']):\n",
    "            temp_pred_score = []\n",
    "            for theme in tsnmf_context['tsnmf_model'].test_data.iloc[ind]['theme']:\n",
    "                theme_id = themes.index(theme)\n",
    "                temp_pred_score.append(np.log(len(themes))-np.log(np.argwhere(doc_wth.argsort()[::-1]==theme_id)[0][0] + 1))\n",
    "            temp_pred_scores.append(temp_pred_score)\n",
    "\n",
    "        prediction_scores.append(np.array([sum(tps) for tps in temp_pred_scores]))\n",
    "\n",
    "    return np.array(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_\"+dataset_name)):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_\"+dataset_name+\"/\"+pickle_name,\"rb\")\n",
    "    tsnmf_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "#     if tsnmf_context_list[0]['tsnmf_model'].bCool_init  and tsnmf_context_list[0]['tsnmf_model'].supervision == 'supervised' and tsnmf_context_list[0]['tsnmf_model'].separate_models:\n",
    "    prediction_scores = calculate_test_score(tsnmf_context_list)\n",
    "    all_models.append(tsnmf_context_list[np.array([np.sum(ps) for ps in prediction_scores]).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'schwartz_terms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 schwartz_nmf_kl_tf_100.pickle\n",
      "1 schwartz_topic1_supervised_separate_bCool_kullback-leibler_tf_100.pickle\n",
      "2 schwartz_topic_supervised_separate_bCool_kullback-leibler_tf_100.pickle\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for idx, pickle_name in enumerate(os.listdir(\"all_pickles/pickles_\"+dataset_name)):\n",
    "    print(idx, pickle_name)\n",
    "    pickle_in = open(\"all_pickles/pickles_\"+dataset_name+\"/\"+pickle_name,\"rb\")\n",
    "    tsnmf_context_list = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    all_models.append(tsnmf_context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_states = []\n",
    "# for i in range(27, 36):\n",
    "#     tsnmf_context_list = all_models[i]\n",
    "#     prediction_scores = calculate_test_score(tsnmf_context_list)\n",
    "#     random_states.append(np.array([np.sum(ps) for ps in prediction_scores]).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models_terms = []\n",
    "# for idx, model in enumerate(all_models):\n",
    "#     all_models_terms.append(model[random_states[idx%9]])\n",
    "all_models_terms = [am[0] for am in all_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row_norm(dists):\n",
    "    # row normalization function required\n",
    "    # for doc_topic_dists and topic_term_dists\n",
    "    return dists / dists.sum(axis=1)[:, None]\n",
    "\n",
    "\n",
    "def _direct_term_scores(W, H, X):\n",
    "    '''\n",
    "    W: doc_topic_dists (n_doc x n_topic)\n",
    "    H: topic_term_dists (n_topic x n_term)\n",
    "    H: doc_topic_dists (n_doc x n_term)\n",
    "    '''\n",
    "    \n",
    "    direct_term_scores = []\n",
    "    \n",
    "    for i, W_row in enumerate(W):\n",
    "        direct_term_scores.append((X[i].toarray()>0) * (H * W_row[:, None]))\n",
    "    \n",
    "    return np.array(direct_term_scores)\n",
    "\n",
    "def _purity_term_scores(direct_term_scores):\n",
    "    purity_term_scores = []\n",
    "    \n",
    "    for i in range(len(direct_term_scores)):\n",
    "        purity_term_scores.append(direct_term_scores[i] * (direct_term_scores[i] / (direct_term_scores[i]+direct_term_scores[-1])))\n",
    "        \n",
    "    return np.nan_to_num(np.array(purity_term_scores))\n",
    "\n",
    "\n",
    "def calculate_term_scores(H, purity_ratio):\n",
    "    direct_term_scores = _row_norm(H)\n",
    "    #direct_term_scores = H\n",
    "    purity_term_scores = _purity_term_scores(direct_term_scores)\n",
    "    \n",
    "    term_scores = (1-purity_ratio) * direct_term_scores + purity_ratio * purity_term_scores\n",
    "    \n",
    "    return np.array(term_scores)\n",
    "\n",
    "\n",
    "def calculate_doc_term_scores(W, H, X, purity_ratio):\n",
    "    direct_term_scores = _direct_term_scores(_row_norm(W), _row_norm(H), X)\n",
    "    purity_term_scores = _purity_term_scores(direct_term_scores)\n",
    "    \n",
    "    docs_term_scores = (1-purity_ratio) * direct_term_scores + purity_ratio * purity_term_scores\n",
    "    \n",
    "    return np.array(docs_term_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(topic_term_dists, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(topic_term_dists):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" - \".join([feature_names[i] + '(' + '%.4f'%topic_term_dists[topic_idx][i] + ')' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "def return_top_words(topic_term_dists, feature_names, n_top_words):\n",
    "    terms_list = []\n",
    "    for topic_idx, topic in enumerate(topic_term_dists):\n",
    "        terms_list.append([feature_names[i] + '(' + '%.4f'%topic_term_dists[topic_idx][i] + ')' for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(all_models_terms[0]['data'].text)\n",
    "term_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "tf = term_vectorizer.fit_transform(corpus)\n",
    "themes = all_models_terms[1]['tsnmf_model'].themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(model):\n",
    "    if 'tsnmf_model' in model:\n",
    "        info_dict = {\n",
    "            'dataset': dataset_name,\n",
    "            'type': 'TSNMF',\n",
    "            'train_split': model['tsnmf_model'].train_test_split[0],\n",
    "            'supervision': model['tsnmf_model'].supervision,\n",
    "            'separate_models': 'separate' if model['tsnmf_model'].separate_models else 'combined',\n",
    "            'subtopic_count': model['W_list'][0].shape[1]-1,\n",
    "            'theme_count': len(model['tsnmf_model'].themes),\n",
    "            'train_set_size': len(model['tsnmf_model'].train_data),\n",
    "            'test_set_size': len(model['tsnmf_model'].test_data),\n",
    "            'bCool_init': 'bCool' if model['tsnmf_model'].bCool_init else 'random',\n",
    "            'beta_loss': model['tsnmf_model'].beta_loss,\n",
    "            'term_vectorizer': model['tsnmf_model'].term_vectorizer,\n",
    "            'solution': '-'\n",
    "        }\n",
    "    else:\n",
    "        model['tf'] = tf\n",
    "        model['tf_vectorizer'] = term_vectorizer\n",
    "        info_dict = {\n",
    "            'dataset': dataset_name,\n",
    "            'type': model['type'],\n",
    "            'train_split': model['train_perc'],\n",
    "            'supervision': '-',\n",
    "            'separate_models': '-',\n",
    "            'subtopic_count': 1,\n",
    "            'theme_count': model['W'].shape[1],\n",
    "            'train_set_size': len(model['data']),\n",
    "            'test_set_size': len(model['data'][model['data']['labeled'] == 0]),\n",
    "            'bCool_init': '-',\n",
    "            'beta_loss': model['beta_loss'],\n",
    "            'term_vectorizer': model['term_vectorizer'],\n",
    "            'solution': model['solution'] \n",
    "        }\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_scores(model, info_dict, purity_ratio, theme_id=0):\n",
    "    if 'tsnmf_model' in model:\n",
    "        if info_dict['separate_models'] == 'combined':\n",
    "            term_scores = calculate_term_scores(model['tsnmf'].components_, purity_ratio=purity_ratio)\n",
    "            doc_term_scores = calculate_doc_term_scores(model['W'], model['tsnmf'].components_, model['tf'], purity_ratio=purity_ratio)\n",
    "        else:\n",
    "            term_scores = calculate_term_scores(model['tsnmf_list'][theme_id].components_, purity_ratio=purity_ratio)\n",
    "            doc_term_scores = calculate_doc_term_scores(model['W_list'][theme_id], model['tsnmf_list'][theme_id].components_, model['tf'], purity_ratio=purity_ratio)\n",
    "    else:\n",
    "        term_scores = calculate_term_scores(model['model'].components_, purity_ratio=0)\n",
    "        doc_term_scores = calculate_doc_term_scores(model['W'], model['model'].components_, model['tf'], purity_ratio=0)\n",
    "        \n",
    "    return term_scores, doc_term_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "purity_ratio = 0\n",
    "with pd.ExcelWriter('schwartz_terms_purity'+str(purity_ratio)+'.xlsx') as writer:\n",
    "    term_count = 20\n",
    "    for model in all_models_terms:\n",
    "        result_df = pd.DataFrame()\n",
    "        info_dict = get_info(model)\n",
    "        if 'tsnmf_model' in model and info_dict['separate_models'] == 'separate':\n",
    "            for theme_id, theme in enumerate(themes):\n",
    "                term_scores, _ = get_term_scores(model, info_dict, purity_ratio, theme_id)\n",
    "\n",
    "                terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "                terms_list_sum = return_top_words(np.sum(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "                terms_list_max = return_top_words(np.max(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "\n",
    "                for i, terms in enumerate(terms_list):\n",
    "                    result_df.insert(len(result_df.columns), theme+'_'+str(i+1), terms)\n",
    "                result_df.insert(len(result_df.columns), theme+'_sum', terms_list_sum)\n",
    "                result_df.insert(len(result_df.columns), theme+'_max', terms_list_max)\n",
    "\n",
    "        else:\n",
    "            term_scores, _ = get_term_scores(model, info_dict, 0)\n",
    "            terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "            for subtopic_id, terms in enumerate(terms_list):\n",
    "                result_df.insert(len(result_df.columns), themes[subtopic_id], terms)\n",
    "        sheet_name = info_dict['type'] + '_' + str(info_dict['supervision']) + '_' + str(info_dict['separate_models']) + '_' + str(info_dict['train_split']) + '_' + str(info_dict['subtopic_count'])\n",
    "        result_df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "term_count = 10\n",
    "model = all_models_terms[0]\n",
    "result_df = pd.DataFrame()\n",
    "info_dict = get_info(model)\n",
    "\n",
    "if 'tsnmf_model' in model and info_dict['separate_models'] == 'separate':\n",
    "    for theme_id, theme in enumerate(themes):\n",
    "        term_scores, _ = get_term_scores(model, info_dict, purity_ratio, theme_id)\n",
    "\n",
    "        terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "        terms_list_sum = return_top_words(np.sum(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "        terms_list_max = return_top_words(np.max(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "\n",
    "        for i, terms in enumerate(terms_list):\n",
    "            result_df.insert(len(result_df.columns), theme+'_'+str(i+1), terms)\n",
    "        result_df.insert(len(result_df.columns), theme+'_sum', terms_list_sum)\n",
    "        result_df.insert(len(result_df.columns), theme+'_max', terms_list_max)\n",
    "\n",
    "else:\n",
    "    term_scores, _ = get_term_scores(model, info_dict, 0)\n",
    "    terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "    for subtopic_id, terms in enumerate(terms_list):\n",
    "        result_df.insert(len(result_df.columns), themes[model['solution'].index(subtopic_id)], terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hedonism</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>power</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>conformity</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>universalism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>tradition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may(0.0074)</td>\n",
       "      <td>marriage(0.0335)</td>\n",
       "      <td>miss(0.0729)</td>\n",
       "      <td>social(0.0307)</td>\n",
       "      <td>environmental(0.0239)</td>\n",
       "      <td>god(0.0149)</td>\n",
       "      <td>theory(0.0182)</td>\n",
       "      <td>peace(0.0255)</td>\n",
       "      <td>work(0.0160)</td>\n",
       "      <td>also(0.0083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social(0.0074)</td>\n",
       "      <td>samesex(0.0283)</td>\n",
       "      <td>little(0.0706)</td>\n",
       "      <td>party(0.0163)</td>\n",
       "      <td>resource(0.0107)</td>\n",
       "      <td>good(0.0115)</td>\n",
       "      <td>creativity(0.0173)</td>\n",
       "      <td>right(0.0212)</td>\n",
       "      <td>status(0.0095)</td>\n",
       "      <td>one(0.0079)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one(0.0071)</td>\n",
       "      <td>state(0.0204)</td>\n",
       "      <td>little miss(0.0670)</td>\n",
       "      <td>political(0.0125)</td>\n",
       "      <td>environment(0.0093)</td>\n",
       "      <td>moral(0.0106)</td>\n",
       "      <td>think(0.0173)</td>\n",
       "      <td>security(0.0179)</td>\n",
       "      <td>time(0.0081)</td>\n",
       "      <td>world(0.0077)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people(0.0067)</td>\n",
       "      <td>samesex marriage(0.0183)</td>\n",
       "      <td>shyness(0.0125)</td>\n",
       "      <td>socialist(0.0121)</td>\n",
       "      <td>ecology(0.0089)</td>\n",
       "      <td>one(0.0101)</td>\n",
       "      <td>capital(0.0141)</td>\n",
       "      <td>international(0.0142)</td>\n",
       "      <td>energy(0.0081)</td>\n",
       "      <td>use(0.0058)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>individual(0.0065)</td>\n",
       "      <td>law(0.0155)</td>\n",
       "      <td>series(0.0114)</td>\n",
       "      <td>socialism(0.0120)</td>\n",
       "      <td>human(0.0082)</td>\n",
       "      <td>law(0.0092)</td>\n",
       "      <td>process(0.0125)</td>\n",
       "      <td>state(0.0141)</td>\n",
       "      <td>use(0.0076)</td>\n",
       "      <td>include(0.0058)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>also(0.0060)</td>\n",
       "      <td>court(0.0128)</td>\n",
       "      <td>book(0.0110)</td>\n",
       "      <td>economic(0.0113)</td>\n",
       "      <td>development(0.0082)</td>\n",
       "      <td>virtue(0.0078)</td>\n",
       "      <td>idea(0.0103)</td>\n",
       "      <td>war(0.0138)</td>\n",
       "      <td>income(0.0073)</td>\n",
       "      <td>love(0.0056)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>use(0.0051)</td>\n",
       "      <td>yes(0.0116)</td>\n",
       "      <td>shy(0.0106)</td>\n",
       "      <td>society(0.0109)</td>\n",
       "      <td>specie(0.0081)</td>\n",
       "      <td>ethic(0.0078)</td>\n",
       "      <td>truth(0.0094)</td>\n",
       "      <td>human(0.0133)</td>\n",
       "      <td>hour(0.0069)</td>\n",
       "      <td>new(0.0055)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>study(0.0048)</td>\n",
       "      <td>independence(0.0110)</td>\n",
       "      <td>title(0.0095)</td>\n",
       "      <td>labour(0.0106)</td>\n",
       "      <td>natural(0.0081)</td>\n",
       "      <td>human(0.0059)</td>\n",
       "      <td>knowledge(0.0093)</td>\n",
       "      <td>unite(0.0114)</td>\n",
       "      <td>system(0.0064)</td>\n",
       "      <td>tradition(0.0051)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>need(0.0046)</td>\n",
       "      <td>union(0.0110)</td>\n",
       "      <td>madame(0.0087)</td>\n",
       "      <td>state(0.0104)</td>\n",
       "      <td>use(0.0072)</td>\n",
       "      <td>philosophy(0.0059)</td>\n",
       "      <td>intelligence(0.0090)</td>\n",
       "      <td>woman(0.0100)</td>\n",
       "      <td>orgasm(0.0058)</td>\n",
       "      <td>day(0.0047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>group(0.0045)</td>\n",
       "      <td>unite(0.0105)</td>\n",
       "      <td>french(0.0087)</td>\n",
       "      <td>anarchist(0.0097)</td>\n",
       "      <td>water(0.0065)</td>\n",
       "      <td>justice(0.0055)</td>\n",
       "      <td>reason(0.0085)</td>\n",
       "      <td>nation(0.0099)</td>\n",
       "      <td>class(0.0057)</td>\n",
       "      <td>century(0.0047)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             hedonism            self-direction                power  \\\n",
       "0         may(0.0074)          marriage(0.0335)         miss(0.0729)   \n",
       "1      social(0.0074)           samesex(0.0283)       little(0.0706)   \n",
       "2         one(0.0071)             state(0.0204)  little miss(0.0670)   \n",
       "3      people(0.0067)  samesex marriage(0.0183)      shyness(0.0125)   \n",
       "4  individual(0.0065)               law(0.0155)       series(0.0114)   \n",
       "5        also(0.0060)             court(0.0128)         book(0.0110)   \n",
       "6         use(0.0051)               yes(0.0116)          shy(0.0106)   \n",
       "7       study(0.0048)      independence(0.0110)        title(0.0095)   \n",
       "8        need(0.0046)             union(0.0110)       madame(0.0087)   \n",
       "9       group(0.0045)             unite(0.0105)       french(0.0087)   \n",
       "\n",
       "         stimulation               security          conformity  \\\n",
       "0     social(0.0307)  environmental(0.0239)         god(0.0149)   \n",
       "1      party(0.0163)       resource(0.0107)        good(0.0115)   \n",
       "2  political(0.0125)    environment(0.0093)       moral(0.0106)   \n",
       "3  socialist(0.0121)        ecology(0.0089)         one(0.0101)   \n",
       "4  socialism(0.0120)          human(0.0082)         law(0.0092)   \n",
       "5   economic(0.0113)    development(0.0082)      virtue(0.0078)   \n",
       "6    society(0.0109)         specie(0.0081)       ethic(0.0078)   \n",
       "7     labour(0.0106)        natural(0.0081)       human(0.0059)   \n",
       "8      state(0.0104)            use(0.0072)  philosophy(0.0059)   \n",
       "9  anarchist(0.0097)          water(0.0065)     justice(0.0055)   \n",
       "\n",
       "            benevolence           universalism     achievement  \\\n",
       "0        theory(0.0182)          peace(0.0255)    work(0.0160)   \n",
       "1    creativity(0.0173)          right(0.0212)  status(0.0095)   \n",
       "2         think(0.0173)       security(0.0179)    time(0.0081)   \n",
       "3       capital(0.0141)  international(0.0142)  energy(0.0081)   \n",
       "4       process(0.0125)          state(0.0141)     use(0.0076)   \n",
       "5          idea(0.0103)            war(0.0138)  income(0.0073)   \n",
       "6         truth(0.0094)          human(0.0133)    hour(0.0069)   \n",
       "7     knowledge(0.0093)          unite(0.0114)  system(0.0064)   \n",
       "8  intelligence(0.0090)          woman(0.0100)  orgasm(0.0058)   \n",
       "9        reason(0.0085)         nation(0.0099)   class(0.0057)   \n",
       "\n",
       "           tradition  \n",
       "0       also(0.0083)  \n",
       "1        one(0.0079)  \n",
       "2      world(0.0077)  \n",
       "3        use(0.0058)  \n",
       "4    include(0.0058)  \n",
       "5       love(0.0056)  \n",
       "6        new(0.0055)  \n",
       "7  tradition(0.0051)  \n",
       "8        day(0.0047)  \n",
       "9    century(0.0047)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datathon]",
   "language": "python",
   "name": "conda-env-datathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
