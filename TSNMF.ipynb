{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Supervised Nonnegative Matrix Factorization for Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import libs.text_preprocess as tp\n",
    "import libs.genetic_algorithm as ga\n",
    "from libs.TSNMF_Class import TSNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "\n",
    "Run one of the cells below\n",
    "\n",
    "* index\n",
    "* theme\n",
    "* text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data['theme'] = data['theme'].apply(lambda x: [x])\n",
    "    data = data.sort_values('theme')\n",
    "    data = data[['title', 'theme', 'text']]\n",
    "    data = data.rename({'title': 'id'}, axis=1)\n",
    "    \n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)\n",
    "\n",
    "dataset = 'schwartz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = reuters.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(reuters.categories(file))\n",
    "    text.append(' '.join(reuters.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)\n",
    "\n",
    "dataset = 'reuters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fileids from the reuters corpus\n",
    "fileids = brown.fileids()\n",
    "\n",
    "# Initialize empty lists to store categories and raw text\n",
    "categories = []\n",
    "text = []\n",
    "\n",
    "# Loop through each file id and collect each files categories and raw text\n",
    "for file in fileids:\n",
    "    categories.append(brown.categories(file))\n",
    "    text.append(' '.join(brown.words(file)))\n",
    "\n",
    "# Combine lists into pandas dataframe. reutersDf is the final dataframe. \n",
    "data = pd.DataFrame({'id':fileids, 'theme':categories, 'text':text}).sort_values('theme').reset_index(drop=True)\n",
    "\n",
    "dataset = 'brown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Texts\n",
    "1. Fix bad wording: isn't -> is not\n",
    "2. Clean and Tokenize -> min word len = 3, tokenize\n",
    "3. Stopword Removal -> nltk.stopwords\n",
    "4. Lemmatization -> WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.text.apply(tp.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['text'].apply(lambda x: x.count(' ') > 25)].reset_index(drop=True).copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TSNMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.. 11.95s - Testing.. 18.19s - Done:  30.17s\n"
     ]
    }
   ],
   "source": [
    "tsnmf_model = TSNMF(data = data, supervision = 'supervised', separate_models = True, bCool_init = True, train_test_split = [0.4, 0.6],\n",
    "                    n_topics = 3, n_terms = 10000, background_for_theme = True, background_scoring = True,\n",
    "                    beta_loss = 'kullback-leibler', term_vectorizer = 'tf', random_state = 6)\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "tsnmf_model = tsnmf_model.split_train_test()\n",
    "\n",
    "print(\"Training.. \", end='')\n",
    "t1 = time()\n",
    "tsnmf_context = tsnmf_model.fit()\n",
    "print(\"%0.2fs - \" % (time() - t1), end='')\n",
    "\n",
    "print(\"Testing.. \", end='')\n",
    "t1 = time()\n",
    "tsnmf_context = tsnmf_model.evaluate_test_corpus(tsnmf_context)\n",
    "print(\"%0.2fs - \" % (time() - t1), end='')\n",
    "\n",
    "tsnmf_context['tsnmf_model'] = tsnmf_model\n",
    "print(\"Done:  %0.2fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_score(tsnmf_context):\n",
    "    themes = tsnmf_context['tsnmf_model'].themes\n",
    "    \n",
    "    temp_pred_scores = []\n",
    "    for ind, doc_wth in enumerate(tsnmf_context['W_test_high']):\n",
    "        temp_pred_score = []\n",
    "        for theme in tsnmf_context['tsnmf_model'].test_data.iloc[ind]['theme']:\n",
    "            theme_id = themes.index(theme)\n",
    "            temp_pred_score.append(np.log(len(themes))-np.log(np.argwhere(doc_wth.argsort()[::-1]==theme_id)[0][0] + 1))\n",
    "        temp_pred_scores.append(temp_pred_score)\n",
    "\n",
    "    return np.array([sum(tps) for tps in temp_pred_scores])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_keys = ['dataset','theme_count','train_set_size','test_set_size','train_split','supervision','separate_models','bCool_init',\n",
    "                    'beta_loss','term_vectorizer','max_score_one_theme','max_score','prediction_score','prediction_score_perc','prediction_score_mean',\n",
    "                    'prediction_score_std']\n",
    "\n",
    "result_df = pd.DataFrame(columns=result_dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = calculate_test_score(tsnmf_context)\n",
    "\n",
    "n_themes = len(tsnmf_context['tsnmf_model'].themes)\n",
    "\n",
    "result_dict = {\n",
    "    'dataset': dataset,\n",
    "    'theme_count': n_themes,\n",
    "    'train_set_size': len(tsnmf_context['tsnmf_model'].train_data),\n",
    "    'test_set_size': len(tsnmf_context['tsnmf_model'].test_data),\n",
    "    'train_split': tsnmf_context['tsnmf_model'].train_test_split[0],\n",
    "    'supervision': tsnmf_context['tsnmf_model'].supervision,\n",
    "    'separate_models': 'separate' if tsnmf_context['tsnmf_model'].separate_models else 'combined',\n",
    "    'bCool_init': 'bCool' if tsnmf_context['tsnmf_model'].bCool_init else 'random',\n",
    "    'beta_loss': tsnmf_context['tsnmf_model'].beta_loss,\n",
    "    'term_vectorizer': tsnmf_context['tsnmf_model'].term_vectorizer,\n",
    "    'max_score_one_theme': np.log(n_themes),\n",
    "    'max_score': 0,\n",
    "    'prediction_score': 0,\n",
    "    'prediction_score_perc': 0,\n",
    "    'prediction_score_mean': 0,\n",
    "    'prediction_score_std': 0\n",
    "}\n",
    "\n",
    "max_score = sum([sum(np.log(n_themes)-np.log(range(1, len(aps)+1))) for aps in tsnmf_context['tsnmf_model'].test_data['theme']])\n",
    "result_dict['max_score'] = max_score\n",
    "result_dict['prediction_score'] = prediction_scores.sum()\n",
    "result_dict['prediction_score_perc'] = 100*prediction_scores.sum()/max_score\n",
    "result_dict['prediction_score_mean'] = prediction_scores.mean()\n",
    "result_dict['prediction_score_std'] = prediction_scores.std()\n",
    "\n",
    "result_df = result_df.append(result_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>theme_count</th>\n",
       "      <th>train_set_size</th>\n",
       "      <th>test_set_size</th>\n",
       "      <th>train_split</th>\n",
       "      <th>supervision</th>\n",
       "      <th>separate_models</th>\n",
       "      <th>bCool_init</th>\n",
       "      <th>beta_loss</th>\n",
       "      <th>term_vectorizer</th>\n",
       "      <th>max_score_one_theme</th>\n",
       "      <th>max_score</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prediction_score_perc</th>\n",
       "      <th>prediction_score_mean</th>\n",
       "      <th>prediction_score_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schwartz</td>\n",
       "      <td>10</td>\n",
       "      <td>173</td>\n",
       "      <td>260</td>\n",
       "      <td>0.4</td>\n",
       "      <td>supervised</td>\n",
       "      <td>separate</td>\n",
       "      <td>bCool</td>\n",
       "      <td>kullback-leibler</td>\n",
       "      <td>tf</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>598.672124</td>\n",
       "      <td>480.543134</td>\n",
       "      <td>80.268166</td>\n",
       "      <td>1.848243</td>\n",
       "      <td>0.645351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset theme_count train_set_size test_set_size  train_split supervision  \\\n",
       "0  schwartz          10            173           260          0.4  supervised   \n",
       "\n",
       "  separate_models bCool_init         beta_loss term_vectorizer  \\\n",
       "0        separate      bCool  kullback-leibler              tf   \n",
       "\n",
       "   max_score_one_theme   max_score  prediction_score  prediction_score_perc  \\\n",
       "0             2.302585  598.672124        480.543134              80.268166   \n",
       "\n",
       "   prediction_score_mean  prediction_score_std  \n",
       "0               1.848243              0.645351  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical NMF-LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.. 20.20s - Genetic Algorithm.. 209.57, 11.20s - Done:  31.42s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "tsnmf_model = TSNMF(data = data, supervision = 'semi_supervised', train_test_split = [0.4, 0.6], random_state = 6)\n",
    "\n",
    "if tsnmf_model.train_test_split[0] > 0.25 and tsnmf_model.train_test_split[0] < 0.75:\n",
    "    tsnmf_model = tsnmf_model.split_train_test()\n",
    "else:\n",
    "    tsnmf_model = tsnmf_model.split_train_test_forced()\n",
    "\n",
    "train_data = tsnmf_model.train_data.copy(deep=True)\n",
    "test_data = tsnmf_model.test_data.copy(deep=True)\n",
    "\n",
    "print(\"Training.. \", end='')\n",
    "t1 = time()\n",
    "\n",
    "corpus = list(train_data.text)\n",
    "term_vectorizer = CountVectorizer(min_df=1, ngram_range=(1,3), max_features=10000)\n",
    "tf = term_vectorizer.fit_transform(corpus)\n",
    "\n",
    "model = NMF(n_components = len(tsnmf_model.themes), solver='mu', beta_loss='kullback-leibler', alpha=.1, l1_ratio=.5)\n",
    "W = model.fit_transform(X=tf)\n",
    "model_type = 'NMF'\n",
    "\n",
    "#model = LDA(n_components=len(tsnmf_model.themes))\n",
    "#W = model.fit_transform(tf)\n",
    "#model_type = 'LDA'\n",
    "\n",
    "print(\"%0.2fs - \" % (time() - t1), end='')\n",
    "\n",
    "print(\"Genetic Algorithm.. \", end='')\n",
    "t1 = time()\n",
    "\n",
    "population, solution, solution_ind, solution_obj = ga.run_ga(train_data[train_data.labeled == 1], W, tsnmf_model.themes, stopGeneration=100)\n",
    "\n",
    "print(\"%0.2f, %0.2fs - \" % (solution_obj, time() - t1), end='')\n",
    "\n",
    "nmflda_context = {\n",
    "    'type': model_type,\n",
    "    'model': model,\n",
    "    'W': W,\n",
    "    'solution': solution,\n",
    "    'solution_obj': solution_obj,\n",
    "    'data': train_data,\n",
    "    'beta_loss': 'none',\n",
    "    'term_vectorizer': 'tf',\n",
    "    'train_perc': tsnmf_model.train_test_split[0],\n",
    "    'random_state': tsnmf_model.random_state\n",
    "}\n",
    "\n",
    "print(\"Done:  %0.2fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_keys = ['type', 'dataset','theme_count','train_set_size','test_set_size','train_split', 'term_vectorizer','max_score_one_theme',\n",
    "                    'max_score','prediction_score','prediction_score_perc','prediction_score_mean', 'prediction_score_std']\n",
    "\n",
    "result_df = pd.DataFrame(columns=result_dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = sorted(list(set(nmflda_context['data']['theme'].sum())))\n",
    "n_themes = len(themes)\n",
    "result_dict = {\n",
    "    'type': nmflda_context['type'],\n",
    "    'dataset': dataset,\n",
    "    'theme_count': n_themes,\n",
    "    'train_set_size': len(nmflda_context['data'][nmflda_context['data'].labeled == 1]),\n",
    "    'test_set_size': len(nmflda_context['data'][nmflda_context['data'].labeled == 0]),\n",
    "    'train_split': nmflda_context['train_perc'],\n",
    "    'term_vectorizer': nmflda_context['term_vectorizer'],\n",
    "    'max_score_one_theme': np.log(n_themes),\n",
    "    'max_score': 0,\n",
    "    'prediction_score': 0,\n",
    "    'prediction_score_perc': 0,\n",
    "    'prediction_score_mean': 0,\n",
    "    'prediction_score_std': 0\n",
    "}\n",
    "\n",
    "prediction_scores, _ = ga.calculateTestScore(nmflda_context['solution'], nmflda_context['data'][nmflda_context['data'].labeled == 0], nmflda_context['W'], themes)\n",
    "prediction_scores = np.array(prediction_scores)\n",
    "\n",
    "max_score = sum([sum(np.log(n_themes)-np.log(range(1, len(aps)+1))) for aps in nmflda_context['data'][nmflda_context['data'].labeled == 0]['theme']])\n",
    "result_dict['max_score'] = max_score\n",
    "result_dict['prediction_score'] = prediction_scores[i].sum()\n",
    "result_dict['prediction_score_perc'] = 100*prediction_scores[i].sum()/max_score\n",
    "result_dict['prediction_score_mean'] = prediction_scores[i].mean()\n",
    "result_dict['prediction_score_std'] = prediction_scores[i].std()\n",
    "\n",
    "result_df = result_df.append(result_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>theme_count</th>\n",
       "      <th>train_set_size</th>\n",
       "      <th>test_set_size</th>\n",
       "      <th>train_split</th>\n",
       "      <th>term_vectorizer</th>\n",
       "      <th>max_score_one_theme</th>\n",
       "      <th>max_score</th>\n",
       "      <th>prediction_score</th>\n",
       "      <th>prediction_score_perc</th>\n",
       "      <th>prediction_score_mean</th>\n",
       "      <th>prediction_score_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NMF</td>\n",
       "      <td>schwartz</td>\n",
       "      <td>10</td>\n",
       "      <td>173</td>\n",
       "      <td>260</td>\n",
       "      <td>0.4</td>\n",
       "      <td>tf</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>598.672124</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.268835</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type   dataset theme_count train_set_size test_set_size  train_split  \\\n",
       "0  NMF  schwartz          10            173           260          0.4   \n",
       "\n",
       "  term_vectorizer  max_score_one_theme   max_score  prediction_score  \\\n",
       "0              tf             2.302585  598.672124          1.609438   \n",
       "\n",
       "   prediction_score_perc  prediction_score_mean  prediction_score_std  \n",
       "0               0.268835               1.609438                   0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row_norm(dists):\n",
    "    # row normalization function required\n",
    "    # for doc_topic_dists and topic_term_dists\n",
    "    return dists / dists.sum(axis=1)[:, None]\n",
    "\n",
    "\n",
    "def _direct_term_scores(W, H, X):\n",
    "    '''\n",
    "    W: doc_topic_dists (n_doc x n_topic)\n",
    "    H: topic_term_dists (n_topic x n_term)\n",
    "    H: doc_topic_dists (n_doc x n_term)\n",
    "    '''\n",
    "    \n",
    "    direct_term_scores = []\n",
    "    \n",
    "    for i, W_row in enumerate(W):\n",
    "        direct_term_scores.append((X[i].toarray()>0) * (H * W_row[:, None]))\n",
    "    \n",
    "    return np.array(direct_term_scores)\n",
    "\n",
    "def _purity_term_scores(direct_term_scores):\n",
    "    purity_term_scores = []\n",
    "    \n",
    "    for i in range(len(direct_term_scores)):\n",
    "        purity_term_scores.append(direct_term_scores[i] * (direct_term_scores[i] / (direct_term_scores[i]+direct_term_scores[-1])))\n",
    "        \n",
    "    return np.nan_to_num(np.array(purity_term_scores))\n",
    "\n",
    "\n",
    "def calculate_term_scores(H, purity_ratio):\n",
    "    direct_term_scores = _row_norm(H)\n",
    "    #direct_term_scores = H\n",
    "    purity_term_scores = _purity_term_scores(direct_term_scores)\n",
    "    \n",
    "    term_scores = (1-purity_ratio) * direct_term_scores + purity_ratio * purity_term_scores\n",
    "    \n",
    "    return np.array(term_scores)\n",
    "\n",
    "\n",
    "def calculate_doc_term_scores(W, H, X, purity_ratio):\n",
    "    direct_term_scores = _direct_term_scores(_row_norm(W), _row_norm(H), X)\n",
    "    purity_term_scores = _purity_term_scores(direct_term_scores)\n",
    "    \n",
    "    docs_term_scores = (1-purity_ratio) * direct_term_scores + purity_ratio * purity_term_scores\n",
    "    \n",
    "    return np.array(docs_term_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(topic_term_dists, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(topic_term_dists):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" - \".join([feature_names[i] + '(' + '%.4f'%topic_term_dists[topic_idx][i] + ')' for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "def return_top_words(topic_term_dists, feature_names, n_top_words):\n",
    "    terms_list = []\n",
    "    for topic_idx, topic in enumerate(topic_term_dists):\n",
    "        terms_list.append([feature_names[i] + '(' + '%.4f'%topic_term_dists[topic_idx][i] + ')' for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return terms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(model):\n",
    "    if 'tsnmf_model' in model:\n",
    "        info_dict = {\n",
    "            'dataset': dataset,\n",
    "            'type': 'TSNMF',\n",
    "            'train_split': model['tsnmf_model'].train_test_split[0],\n",
    "            'supervision': model['tsnmf_model'].supervision,\n",
    "            'separate_models': 'separate' if model['tsnmf_model'].separate_models else 'combined',\n",
    "            'subtopic_count': model['W_list'][0].shape[1]-1,\n",
    "            'theme_count': len(model['tsnmf_model'].themes),\n",
    "            'train_set_size': len(model['tsnmf_model'].train_data),\n",
    "            'test_set_size': len(model['tsnmf_model'].test_data),\n",
    "            'bCool_init': 'bCool' if model['tsnmf_model'].bCool_init else 'random',\n",
    "            'beta_loss': model['tsnmf_model'].beta_loss,\n",
    "            'term_vectorizer': model['tsnmf_model'].term_vectorizer,\n",
    "            'solution': '-'\n",
    "        }\n",
    "    else:\n",
    "        model['tf'] = tf\n",
    "        model['tf_vectorizer'] = term_vectorizer\n",
    "        info_dict = {\n",
    "            'dataset': dataset,\n",
    "            'type': model['type'],\n",
    "            'train_split': model['train_perc'],\n",
    "            'supervision': '-',\n",
    "            'separate_models': '-',\n",
    "            'subtopic_count': 1,\n",
    "            'theme_count': model['W'].shape[1],\n",
    "            'train_set_size': len(model['data']),\n",
    "            'test_set_size': len(model['data'][model['data']['labeled'] == 0]),\n",
    "            'bCool_init': '-',\n",
    "            'beta_loss': model['beta_loss'],\n",
    "            'term_vectorizer': model['term_vectorizer'],\n",
    "            'solution': model['solution'] \n",
    "        }\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_scores(model, info_dict, purity_ratio, theme_id=0):\n",
    "    if 'tsnmf_model' in model:\n",
    "        if info_dict['separate_models'] == 'combined':\n",
    "            term_scores = calculate_term_scores(model['tsnmf'].components_, purity_ratio=purity_ratio)\n",
    "            doc_term_scores = calculate_doc_term_scores(model['W'], model['tsnmf'].components_, model['tf'], purity_ratio=purity_ratio)\n",
    "        else:\n",
    "            term_scores = calculate_term_scores(model['tsnmf_list'][theme_id].components_, purity_ratio=purity_ratio)\n",
    "            doc_term_scores = calculate_doc_term_scores(model['W_list'][theme_id], model['tsnmf_list'][theme_id].components_, model['tf'], purity_ratio=purity_ratio)\n",
    "    else:\n",
    "        term_scores = calculate_term_scores(model['model'].components_, purity_ratio=0)\n",
    "        doc_term_scores = calculate_doc_term_scores(model['W'], model['model'].components_, model['tf'], purity_ratio=0)\n",
    "        \n",
    "    return term_scores, doc_term_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\Anaconda3\\envs\\datathon\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "purity_ratio = 1\n",
    "term_count = 10\n",
    "model = tsnmf_context\n",
    "result_df = pd.DataFrame()\n",
    "info_dict = get_info(model)\n",
    "\n",
    "if 'tsnmf_model' in model and info_dict['separate_models'] == 'separate':\n",
    "    for theme_id, theme in enumerate(themes):\n",
    "        term_scores, _ = get_term_scores(model, info_dict, purity_ratio, theme_id)\n",
    "\n",
    "        terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "        terms_list_sum = return_top_words(np.sum(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "        terms_list_max = return_top_words(np.max(term_scores[:-1],0)[None, :], model['tf_vectorizer'].get_feature_names(), term_count)[0]\n",
    "\n",
    "        for i, terms in enumerate(terms_list):\n",
    "            result_df.insert(len(result_df.columns), theme+'_'+str(i+1), terms)\n",
    "        result_df.insert(len(result_df.columns), theme+'_sum', terms_list_sum)\n",
    "        result_df.insert(len(result_df.columns), theme+'_max', terms_list_max)\n",
    "\n",
    "else:\n",
    "    term_scores, _ = get_term_scores(model, info_dict, 0)\n",
    "    terms_list = return_top_words(term_scores, model['tf_vectorizer'].get_feature_names(), term_count)\n",
    "    for subtopic_id, terms in enumerate(terms_list):\n",
    "        result_df.insert(len(result_df.columns), themes[model['solution'].index(subtopic_id)], terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>achievement_1</th>\n",
       "      <th>achievement_2</th>\n",
       "      <th>achievement_3</th>\n",
       "      <th>achievement_4</th>\n",
       "      <th>achievement_sum</th>\n",
       "      <th>achievement_max</th>\n",
       "      <th>benevolence_1</th>\n",
       "      <th>benevolence_2</th>\n",
       "      <th>benevolence_3</th>\n",
       "      <th>benevolence_4</th>\n",
       "      <th>...</th>\n",
       "      <th>tradition_3</th>\n",
       "      <th>tradition_4</th>\n",
       "      <th>tradition_sum</th>\n",
       "      <th>tradition_max</th>\n",
       "      <th>universalism_1</th>\n",
       "      <th>universalism_2</th>\n",
       "      <th>universalism_3</th>\n",
       "      <th>universalism_4</th>\n",
       "      <th>universalism_sum</th>\n",
       "      <th>universalism_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capital(0.0207)</td>\n",
       "      <td>bystander(0.0100)</td>\n",
       "      <td>status(0.0253)</td>\n",
       "      <td>one(0.0026)</td>\n",
       "      <td>social(0.0365)</td>\n",
       "      <td>status(0.0253)</td>\n",
       "      <td>law(0.0241)</td>\n",
       "      <td>truth(0.0250)</td>\n",
       "      <td>pardon(0.0141)</td>\n",
       "      <td>social(0.0028)</td>\n",
       "      <td>...</td>\n",
       "      <td>humility(0.0495)</td>\n",
       "      <td>social(0.0027)</td>\n",
       "      <td>virtue(0.0507)</td>\n",
       "      <td>humility(0.0495)</td>\n",
       "      <td>environmental(0.0102)</td>\n",
       "      <td>energy(0.0149)</td>\n",
       "      <td>green(0.0105)</td>\n",
       "      <td>one(0.0031)</td>\n",
       "      <td>environmental(0.0184)</td>\n",
       "      <td>energy(0.0149)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social(0.0141)</td>\n",
       "      <td>individual(0.0082)</td>\n",
       "      <td>social(0.0144)</td>\n",
       "      <td>social(0.0022)</td>\n",
       "      <td>status(0.0296)</td>\n",
       "      <td>capital(0.0207)</td>\n",
       "      <td>ethic(0.0166)</td>\n",
       "      <td>theory(0.0106)</td>\n",
       "      <td>rand(0.0096)</td>\n",
       "      <td>one(0.0025)</td>\n",
       "      <td>...</td>\n",
       "      <td>christian(0.0446)</td>\n",
       "      <td>one(0.0026)</td>\n",
       "      <td>tradition(0.0498)</td>\n",
       "      <td>virtue(0.0454)</td>\n",
       "      <td>movement(0.0065)</td>\n",
       "      <td>peace(0.0087)</td>\n",
       "      <td>environmental(0.0082)</td>\n",
       "      <td>social(0.0029)</td>\n",
       "      <td>energy(0.0158)</td>\n",
       "      <td>green(0.0105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stratification(0.0081)</td>\n",
       "      <td>impression(0.0082)</td>\n",
       "      <td>influence(0.0093)</td>\n",
       "      <td>use(0.0021)</td>\n",
       "      <td>capital(0.0213)</td>\n",
       "      <td>social(0.0144)</td>\n",
       "      <td>natural(0.0122)</td>\n",
       "      <td>altruism(0.0088)</td>\n",
       "      <td>loyalty(0.0096)</td>\n",
       "      <td>use(0.0021)</td>\n",
       "      <td>...</td>\n",
       "      <td>view(0.0156)</td>\n",
       "      <td>use(0.0021)</td>\n",
       "      <td>humility(0.0495)</td>\n",
       "      <td>christian(0.0446)</td>\n",
       "      <td>socialism(0.0055)</td>\n",
       "      <td>displaystyle(0.0062)</td>\n",
       "      <td>party(0.0065)</td>\n",
       "      <td>may(0.0025)</td>\n",
       "      <td>green(0.0118)</td>\n",
       "      <td>environmental(0.0102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>society(0.0079)</td>\n",
       "      <td>social(0.0079)</td>\n",
       "      <td>attainment(0.0083)</td>\n",
       "      <td>state(0.0020)</td>\n",
       "      <td>individual(0.0158)</td>\n",
       "      <td>bystander(0.0100)</td>\n",
       "      <td>natural law(0.0111)</td>\n",
       "      <td>true(0.0073)</td>\n",
       "      <td>responsibility(0.0061)</td>\n",
       "      <td>may(0.0020)</td>\n",
       "      <td>...</td>\n",
       "      <td>god(0.0143)</td>\n",
       "      <td>may(0.0020)</td>\n",
       "      <td>christian(0.0456)</td>\n",
       "      <td>tradition(0.0432)</td>\n",
       "      <td>socialist(0.0055)</td>\n",
       "      <td>abduction(0.0047)</td>\n",
       "      <td>think(0.0057)</td>\n",
       "      <td>people(0.0021)</td>\n",
       "      <td>party(0.0104)</td>\n",
       "      <td>peace(0.0087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accumulation(0.0077)</td>\n",
       "      <td>group(0.0068)</td>\n",
       "      <td>ascribe(0.0073)</td>\n",
       "      <td>may(0.0020)</td>\n",
       "      <td>society(0.0152)</td>\n",
       "      <td>influence(0.0093)</td>\n",
       "      <td>good(0.0070)</td>\n",
       "      <td>help(0.0065)</td>\n",
       "      <td>grant(0.0055)</td>\n",
       "      <td>state(0.0020)</td>\n",
       "      <td>...</td>\n",
       "      <td>jesus(0.0121)</td>\n",
       "      <td>state(0.0020)</td>\n",
       "      <td>ethic(0.0299)</td>\n",
       "      <td>ascetic(0.0231)</td>\n",
       "      <td>woman(0.0044)</td>\n",
       "      <td>advaita(0.0041)</td>\n",
       "      <td>convention(0.0051)</td>\n",
       "      <td>also(0.0021)</td>\n",
       "      <td>movement(0.0096)</td>\n",
       "      <td>party(0.0065)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>production(0.0065)</td>\n",
       "      <td>management(0.0065)</td>\n",
       "      <td>society(0.0072)</td>\n",
       "      <td>also(0.0019)</td>\n",
       "      <td>influence(0.0114)</td>\n",
       "      <td>attainment(0.0083)</td>\n",
       "      <td>moral(0.0060)</td>\n",
       "      <td>one(0.0053)</td>\n",
       "      <td>one(0.0051)</td>\n",
       "      <td>also(0.0019)</td>\n",
       "      <td>...</td>\n",
       "      <td>christianity(0.0113)</td>\n",
       "      <td>also(0.0019)</td>\n",
       "      <td>practice(0.0251)</td>\n",
       "      <td>moral(0.0219)</td>\n",
       "      <td>right(0.0043)</td>\n",
       "      <td>peirce(0.0038)</td>\n",
       "      <td>conservation(0.0050)</td>\n",
       "      <td>individual(0.0020)</td>\n",
       "      <td>peace(0.0090)</td>\n",
       "      <td>movement(0.0065)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>class(0.0060)</td>\n",
       "      <td>male(0.0060)</td>\n",
       "      <td>individual(0.0062)</td>\n",
       "      <td>people(0.0017)</td>\n",
       "      <td>people(0.0102)</td>\n",
       "      <td>individual(0.0082)</td>\n",
       "      <td>nature(0.0057)</td>\n",
       "      <td>theory truth(0.0048)</td>\n",
       "      <td>sentence(0.0041)</td>\n",
       "      <td>environmental(0.0018)</td>\n",
       "      <td>...</td>\n",
       "      <td>ethic(0.0106)</td>\n",
       "      <td>people(0.0018)</td>\n",
       "      <td>ascetic(0.0231)</td>\n",
       "      <td>practice(0.0205)</td>\n",
       "      <td>economic(0.0041)</td>\n",
       "      <td>nobel(0.0036)</td>\n",
       "      <td>green party(0.0050)</td>\n",
       "      <td>theory(0.0020)</td>\n",
       "      <td>world(0.0075)</td>\n",
       "      <td>displaystyle(0.0062)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>value(0.0054)</td>\n",
       "      <td>impression management(0.0058)</td>\n",
       "      <td>ascribe status(0.0061)</td>\n",
       "      <td>environmental(0.0017)</td>\n",
       "      <td>group(0.0100)</td>\n",
       "      <td>impression(0.0082)</td>\n",
       "      <td>philosophy(0.0048)</td>\n",
       "      <td>say(0.0035)</td>\n",
       "      <td>diffusion(0.0040)</td>\n",
       "      <td>people(0.0017)</td>\n",
       "      <td>...</td>\n",
       "      <td>love(0.0098)</td>\n",
       "      <td>theory(0.0016)</td>\n",
       "      <td>moral(0.0227)</td>\n",
       "      <td>asceticism(0.0199)</td>\n",
       "      <td>sustainable(0.0040)</td>\n",
       "      <td>form(0.0032)</td>\n",
       "      <td>environment(0.0049)</td>\n",
       "      <td>use(0.0020)</td>\n",
       "      <td>resource(0.0073)</td>\n",
       "      <td>think(0.0057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>economic(0.0053)</td>\n",
       "      <td>dominance(0.0056)</td>\n",
       "      <td>foucault(0.0057)</td>\n",
       "      <td>theory(0.0016)</td>\n",
       "      <td>bystander(0.0100)</td>\n",
       "      <td>stratification(0.0081)</td>\n",
       "      <td>right(0.0045)</td>\n",
       "      <td>love(0.0034)</td>\n",
       "      <td>diffusion responsibility(0.0037)</td>\n",
       "      <td>group(0.0016)</td>\n",
       "      <td>...</td>\n",
       "      <td>life(0.0094)</td>\n",
       "      <td>group(0.0016)</td>\n",
       "      <td>asceticism(0.0199)</td>\n",
       "      <td>ethic(0.0193)</td>\n",
       "      <td>feminist(0.0039)</td>\n",
       "      <td>reality(0.0031)</td>\n",
       "      <td>natural(0.0049)</td>\n",
       "      <td>group(0.0019)</td>\n",
       "      <td>think(0.0071)</td>\n",
       "      <td>socialism(0.0055)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>business(0.0050)</td>\n",
       "      <td>help(0.0052)</td>\n",
       "      <td>status attainment(0.0056)</td>\n",
       "      <td>include(0.0015)</td>\n",
       "      <td>high(0.0100)</td>\n",
       "      <td>society(0.0079)</td>\n",
       "      <td>sin(0.0038)</td>\n",
       "      <td>people(0.0034)</td>\n",
       "      <td>objectivism(0.0036)</td>\n",
       "      <td>individual(0.0016)</td>\n",
       "      <td>...</td>\n",
       "      <td>one(0.0077)</td>\n",
       "      <td>individual(0.0016)</td>\n",
       "      <td>one(0.0197)</td>\n",
       "      <td>character(0.0167)</td>\n",
       "      <td>party(0.0039)</td>\n",
       "      <td>hypothesis(0.0031)</td>\n",
       "      <td>resource(0.0047)</td>\n",
       "      <td>state(0.0018)</td>\n",
       "      <td>use(0.0069)</td>\n",
       "      <td>socialist(0.0055)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            achievement_1                  achievement_2  \\\n",
       "0         capital(0.0207)              bystander(0.0100)   \n",
       "1          social(0.0141)             individual(0.0082)   \n",
       "2  stratification(0.0081)             impression(0.0082)   \n",
       "3         society(0.0079)                 social(0.0079)   \n",
       "4    accumulation(0.0077)                  group(0.0068)   \n",
       "5      production(0.0065)             management(0.0065)   \n",
       "6           class(0.0060)                   male(0.0060)   \n",
       "7           value(0.0054)  impression management(0.0058)   \n",
       "8        economic(0.0053)              dominance(0.0056)   \n",
       "9        business(0.0050)                   help(0.0052)   \n",
       "\n",
       "               achievement_3          achievement_4     achievement_sum  \\\n",
       "0             status(0.0253)            one(0.0026)      social(0.0365)   \n",
       "1             social(0.0144)         social(0.0022)      status(0.0296)   \n",
       "2          influence(0.0093)            use(0.0021)     capital(0.0213)   \n",
       "3         attainment(0.0083)          state(0.0020)  individual(0.0158)   \n",
       "4            ascribe(0.0073)            may(0.0020)     society(0.0152)   \n",
       "5            society(0.0072)           also(0.0019)   influence(0.0114)   \n",
       "6         individual(0.0062)         people(0.0017)      people(0.0102)   \n",
       "7     ascribe status(0.0061)  environmental(0.0017)       group(0.0100)   \n",
       "8           foucault(0.0057)         theory(0.0016)   bystander(0.0100)   \n",
       "9  status attainment(0.0056)        include(0.0015)        high(0.0100)   \n",
       "\n",
       "          achievement_max        benevolence_1         benevolence_2  \\\n",
       "0          status(0.0253)          law(0.0241)         truth(0.0250)   \n",
       "1         capital(0.0207)        ethic(0.0166)        theory(0.0106)   \n",
       "2          social(0.0144)      natural(0.0122)      altruism(0.0088)   \n",
       "3       bystander(0.0100)  natural law(0.0111)          true(0.0073)   \n",
       "4       influence(0.0093)         good(0.0070)          help(0.0065)   \n",
       "5      attainment(0.0083)        moral(0.0060)           one(0.0053)   \n",
       "6      individual(0.0082)       nature(0.0057)  theory truth(0.0048)   \n",
       "7      impression(0.0082)   philosophy(0.0048)           say(0.0035)   \n",
       "8  stratification(0.0081)        right(0.0045)          love(0.0034)   \n",
       "9         society(0.0079)          sin(0.0038)        people(0.0034)   \n",
       "\n",
       "                      benevolence_3          benevolence_4  ...  \\\n",
       "0                    pardon(0.0141)         social(0.0028)  ...   \n",
       "1                      rand(0.0096)            one(0.0025)  ...   \n",
       "2                   loyalty(0.0096)            use(0.0021)  ...   \n",
       "3            responsibility(0.0061)            may(0.0020)  ...   \n",
       "4                     grant(0.0055)          state(0.0020)  ...   \n",
       "5                       one(0.0051)           also(0.0019)  ...   \n",
       "6                  sentence(0.0041)  environmental(0.0018)  ...   \n",
       "7                 diffusion(0.0040)         people(0.0017)  ...   \n",
       "8  diffusion responsibility(0.0037)          group(0.0016)  ...   \n",
       "9               objectivism(0.0036)     individual(0.0016)  ...   \n",
       "\n",
       "            tradition_3         tradition_4       tradition_sum  \\\n",
       "0      humility(0.0495)      social(0.0027)      virtue(0.0507)   \n",
       "1     christian(0.0446)         one(0.0026)   tradition(0.0498)   \n",
       "2          view(0.0156)         use(0.0021)    humility(0.0495)   \n",
       "3           god(0.0143)         may(0.0020)   christian(0.0456)   \n",
       "4         jesus(0.0121)       state(0.0020)       ethic(0.0299)   \n",
       "5  christianity(0.0113)        also(0.0019)    practice(0.0251)   \n",
       "6         ethic(0.0106)      people(0.0018)     ascetic(0.0231)   \n",
       "7          love(0.0098)      theory(0.0016)       moral(0.0227)   \n",
       "8          life(0.0094)       group(0.0016)  asceticism(0.0199)   \n",
       "9           one(0.0077)  individual(0.0016)         one(0.0197)   \n",
       "\n",
       "        tradition_max         universalism_1        universalism_2  \\\n",
       "0    humility(0.0495)  environmental(0.0102)        energy(0.0149)   \n",
       "1      virtue(0.0454)       movement(0.0065)         peace(0.0087)   \n",
       "2   christian(0.0446)      socialism(0.0055)  displaystyle(0.0062)   \n",
       "3   tradition(0.0432)      socialist(0.0055)     abduction(0.0047)   \n",
       "4     ascetic(0.0231)          woman(0.0044)       advaita(0.0041)   \n",
       "5       moral(0.0219)          right(0.0043)        peirce(0.0038)   \n",
       "6    practice(0.0205)       economic(0.0041)         nobel(0.0036)   \n",
       "7  asceticism(0.0199)    sustainable(0.0040)          form(0.0032)   \n",
       "8       ethic(0.0193)       feminist(0.0039)       reality(0.0031)   \n",
       "9   character(0.0167)          party(0.0039)    hypothesis(0.0031)   \n",
       "\n",
       "          universalism_3      universalism_4       universalism_sum  \\\n",
       "0          green(0.0105)         one(0.0031)  environmental(0.0184)   \n",
       "1  environmental(0.0082)      social(0.0029)         energy(0.0158)   \n",
       "2          party(0.0065)         may(0.0025)          green(0.0118)   \n",
       "3          think(0.0057)      people(0.0021)          party(0.0104)   \n",
       "4     convention(0.0051)        also(0.0021)       movement(0.0096)   \n",
       "5   conservation(0.0050)  individual(0.0020)          peace(0.0090)   \n",
       "6    green party(0.0050)      theory(0.0020)          world(0.0075)   \n",
       "7    environment(0.0049)         use(0.0020)       resource(0.0073)   \n",
       "8        natural(0.0049)       group(0.0019)          think(0.0071)   \n",
       "9       resource(0.0047)       state(0.0018)            use(0.0069)   \n",
       "\n",
       "        universalism_max  \n",
       "0         energy(0.0149)  \n",
       "1          green(0.0105)  \n",
       "2  environmental(0.0102)  \n",
       "3          peace(0.0087)  \n",
       "4          party(0.0065)  \n",
       "5       movement(0.0065)  \n",
       "6   displaystyle(0.0062)  \n",
       "7          think(0.0057)  \n",
       "8      socialism(0.0055)  \n",
       "9      socialist(0.0055)  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datathon]",
   "language": "python",
   "name": "conda-env-datathon-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
